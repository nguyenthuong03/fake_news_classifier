{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a8e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cai pakage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "# visual packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# SKLearn pakage\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# NLTK modules\n",
    "import nltk\n",
    "from nltk import FreqDist, WordNetLemmatizer, pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "# Suppress future, deprecation, and SettingWithCopy warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category= FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# make all columns in a df viewable and wider\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.width = None\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed379948",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "realnews = pd.read_csv('data/True.csv')\n",
    "\n",
    "realnews['target'] = 0\n",
    "realnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa99de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakenews = pd.read_csv('data/Fake.csv')\n",
    "\n",
    "fakenews['target'] = 1\n",
    "fakenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fecc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.concat([realnews, fakenews]).reset_index()\n",
    "news.drop(['index', 'subject', 'date'], axis = 1, inplace = True)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1106f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data and eda\n",
    "news['article_length'] = news['text'].str.len()\n",
    "news.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9950c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot article lengths up to a certain threshold\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(data = news, x = 'article_length', hue = 'target')\n",
    "\n",
    "ax.set_title('Partial Histogram of Article Lengths')\n",
    "ax.set_xlabel('Article Length (Characters)')\n",
    "ax.legend(labels = ['Fake', 'Real'])\n",
    "plt.xlim(0, 6000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "news[news['article_length'] <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df983b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "news[news['article_length'] <= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortarticles = news[news['article_length'] <= 50]\n",
    "news.drop(index = shortarticles.index, inplace = True)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36354324",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.loc[news['text'].duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca64cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.drop_duplicates(keep = 'first', inplace = True)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f8bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = news.drop('target', axis = 1)\n",
    "y = news['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"text\"] = X_train[\"text\"].str.lower()\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_links(string):\n",
    "    '''\n",
    "    A function that takes in a string as an input. \n",
    "    \n",
    "    Uses a regex expression to detect URLs that start with http, https, pic.twitter, or www.; \n",
    "    Twitter account names (which start with @); and hashtags (which start with #) and replaces \n",
    "    them with a blank.\n",
    "    \n",
    "    Returns the string with that substitution made.\n",
    "    '''\n",
    "    url_pattern = r'(?:http|https|pic\\.twitter|www\\.)\\S+|@\\S+|^@\\S+|#\\S+'\n",
    "    return re.sub(url_pattern, '', string, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe2252",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train['text'] = X_train['text'].map(replace_links)\n",
    "X_train.sample(n=10, random_state = 270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5864ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc14e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_reuters_mask = realnews['text'].str.contains('Reuters').sum()\n",
    "fake_reuters_mask = fakenews['text'].str.contains('Reuters').sum()\n",
    "\n",
    "print(f'The word Reuters appears in {real_reuters_mask} out of {realnews.shape[0]} total real news articles.')\n",
    "print(f'The word Reuters appears in {fake_reuters_mask} out of {fakenews.shape[0]} total fake news articles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963acd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = X_train['text'].str.replace('reuters', '')\n",
    "X_train.sample(n=10, random_state = 270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_token_pattern = r\"\\b[a-zA-Z]{3,}\\b\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "\n",
    "X_train['tokenized_text'] = X_train['text'].apply(tokenizer.tokenize)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56841129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_top_10(freq_dist, title):\n",
    "\n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(tokens, counts)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis=\"x\", rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_freq_dist = FreqDist(X_train[\"tokenized_text\"].explode())\n",
    "\n",
    "# Plot \n",
    "visualize_top_10(train_freq_dist, \"Top 10 Word Frequency for Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3463a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_Xtrain = pd.Series(list(itertools.chain(*X_train['tokenized_text'])))\n",
    "flattened_Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pd.Series(flattened_Xtrain.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token lists to strings\n",
    "X_baseline_tokens = X_train[\"tokenized_text\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe5b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate, fit, and transform vectorizer\n",
    "tfidf_baseline = TfidfVectorizer()\n",
    "X_baseline_vec = tfidf_baseline.fit_transform(X_baseline_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = MultinomialNB()\n",
    "baseline_cv = cross_val_score(baseline_model, X_baseline_vec, y_train)\n",
    "print(f'CV scores: {baseline_cv.round(4)}')\n",
    "print(f'Mean CV score: {baseline_cv.mean().round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cabfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.append(['also', 'reuters'])\n",
    "\n",
    "def remove_stopwords(token_list):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, return a list where the tokens\n",
    "    that are also present in stopwords_list have been\n",
    "    removed\n",
    "    \"\"\"\n",
    "    return [token for token in token_list if token not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb825286",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['stopwords_removed'] = X_train[\"tokenized_text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da540b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fc298",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stopwords_freq_dist = FreqDist(X_train[\"stopwords_removed\"].explode())\n",
    "\n",
    "visualize_top_10(stopwords_freq_dist, \"Top 10 Words - No Stop Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_nostopwords = pd.Series(list(itertools.chain(*X_train['stopwords_removed'])))\n",
    "print(f'After removing stop words, there are {len(flattened_nostopwords)} tokens in the corpus,\\\n",
    "      {len(flattened_nostopwords.unique())} of which are unique.')\n",
    "print(f'Before removing stop words, there were {len(flattened_Xtrain)} tokens in the corpus,\\\n",
    "      {len(flattened_Xtrain.unique())} of which were unique.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nostopwords_tokens = X_train[\"stopwords_removed\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d6654",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_nostopwords = TfidfVectorizer()\n",
    "X_nostopwords_vec = tfidf_nostopwords.fit_transform(X_nostopwords_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b13d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nostopwords_model = MultinomialNB()\n",
    "nostopwords_cv = cross_val_score(nostopwords_model, X_nostopwords_vec, y_train)\n",
    "print(f'CV scores: {nostopwords_cv.round(4)}')\n",
    "print(f'Mean CV score: {nostopwords_cv.mean().round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    # make all characters lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove URLs, twitter names, etc\n",
    "    url_pattern = r'(?:http|https|pic\\.twitter|www\\.)\\S+|@\\S+|^@\\S+|#\\S+'\n",
    "    string =  re.sub(url_pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # remove Reuters\n",
    "    text = text.replace('reuters', '')\n",
    "    \n",
    "    # tokenize text\n",
    "    basic_token_pattern = r\"\\b[a-zA-Z]{3,}\\b\"\n",
    "    tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = [token.lower() for token in text if token.lower() not in stopwords_list]\n",
    "    \n",
    "    #initialize lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "        \n",
    "    # creates list of tuples with tokens and POS tags in wordnet format\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(text))) \n",
    "    \n",
    "    # lemmatizes each token based on part of speech in tuple\n",
    "    text = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c32772",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['lemmatized'] = X_train['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060eff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_dist = FreqDist(X_train[\"lemmatized\"].explode())\n",
    "visualize_top_10(lemmatized_dist, \"Top 10 Words - Lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f262e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_lemmatization = pd.Series(list(itertools.chain(*X_train['lemmatized'])))\n",
    "print(f'After removing stop words and lemmatizing, there are {len(flattened_lemmatization)} tokens in the corpus,\\\n",
    "      {len(flattened_lemmatization.unique())} of which are unique.')\n",
    "print(f'After removing stop words but before lemmatizing, there were {len(flattened_nostopwords)} tokens in the corpus,\\\n",
    "      {len(flattened_nostopwords.unique())} of which are unique.')\n",
    "print(f'Before removing stop words, there were {len(flattened_Xtrain)} tokens in the corpus,\\\n",
    "      {len(flattened_Xtrain.unique())} of which were unique.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lemm_tokens = X_train[\"lemmatized\"].str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ea07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_lemm = TfidfVectorizer()\n",
    "X_lemm_vec = tfidf_lemm.fit_transform(X_lemm_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_model = MultinomialNB()\n",
    "lemm_cv = cross_val_score(lemm_model, X_lemm_vec, y_train)\n",
    "\n",
    "print(f'CV scores: {lemm_cv.round(4)}')\n",
    "print(f'Mean CV score: {lemm_cv.mean().round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca0dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame({'Model': [],\n",
    "                           'Mean CV Accuracy':[]})\n",
    "\n",
    "evaluation.loc[0] = ['Naive Bayes', lemm_cv.mean().round(4)]\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b4e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_lemm_vec, y_train)\n",
    "lr_cv = cross_val_score(lr_model, X_lemm_vec, y_train)\n",
    "\n",
    "evaluation.loc[1] = ['Logistic Reg.', lr_cv.mean().round(4)]\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = LinearSVC()\n",
    "svc_model.fit(X_lemm_vec, y_train)\n",
    "svc_cv = cross_val_score(svc_model, X_lemm_vec, y_train)\n",
    "\n",
    "evaluation.loc[2] = ['SVC', svc_cv.mean().round(4)]\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea736c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_lemm_vec, y_train)\n",
    "rf_cv = cross_val_score(rf_model, X_lemm_vec, y_train)\n",
    "\n",
    "evaluation.loc[3] = ['Random Forest', rf_cv.mean().round(4)]\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate linear svc and pipeline\n",
    "svc_model_pipe = LinearSVC(class_weight = {0: 0.55, 1: 0.45})\n",
    "svc_pipe = Pipeline([('model', svc_model_pipe)])\n",
    "svc_params = {'model__C': [0.01, 0.1, 1], \n",
    "              'model__tol': [1e-6, 1e-4, 1e-2, 1], \n",
    "              'model__max_iter': [1000, 2000, 3000]}\n",
    "svc_gs = GridSearchCV(estimator = svc_pipe, param_grid = svc_params, \n",
    "                      cv = 5, scoring = 'accuracy', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gs.fit(X_lemm_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e37f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svc_gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ee514",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc_model = svc_gs.best_estimator_\n",
    "\n",
    "best_svc_model.fit(X_lemm_vec, y_train)\n",
    "svc_cv_gs = cross_val_score(best_svc_model, X_lemm_vec, y_train)\n",
    "\n",
    "evaluation.loc[4] = ['Tuned SVC', svc_cv_gs.mean().round(4)]\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['lemmatized'] = X_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997aa7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lemm_tokens = X_test[\"lemmatized\"].str.join(\" \")\n",
    "X_test_lemm_vec = tfidf_lemm.transform(X_test_lemm_tokens)\n",
    "svc_model.score(X_test_lemm_vec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2771e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_svc_model.pkl','wb') as f:\n",
    "    pickle.dump(svc_model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd71e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_lemm.pkl','wb') as e:\n",
    "    pickle.dump(tfidf_lemm,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d897bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(svc_model, X_test_lemm_vec, y_test, \n",
    "                                      display_labels=[\"Real\", \"Fake\"],\n",
    "                                      cmap = 'Blues', ax = ax)\n",
    "\n",
    "ax.set_title('News Classification Predictions - Linear SVC');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, svc_model.predict(X_test_lemm_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_features = pd.DataFrame(zip(tfidf_lemm.get_feature_names_out(), np.transpose(svc_model.coef_)), \n",
    "                           columns=['words', 'coef'])\n",
    "\n",
    "svc_features.sort_values('coef').tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_features.sort_values('coef').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cce99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['lemmatized'] = news['text'].apply(preprocess_text)\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33dc13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_lemm_tokens = news[\"lemmatized\"].str.join(\" \")\n",
    "news_lemm_vec = tfidf_lemm.transform(news_lemm_tokens)\n",
    "news['predictions'] = svc_model.predict(news_lemm_vec)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b767fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(news_lemm_vec.toarray(), columns=tfidf_lemm.get_feature_names_out(), index = news.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbce869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "series = []\n",
    "\n",
    "for x in range(len(df)):\n",
    "    top_words = df.iloc[x]\n",
    "    top = top_words.sort_values(ascending=False)[:1]\n",
    "    one = list(top.index)[0]\n",
    "    series.append(one)\n",
    "news['top_word'] = series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f97cc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767da517",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['top_word'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fddf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide df into true and false positives and negatives\n",
    "realcorrect = news.loc[(news['target'] == 0) & (news['predictions'] == 0)]\n",
    "fakecorrect = news.loc[(news['target'] == 1) & (news['predictions'] == 1)]\n",
    "realwrong = news.loc[(news['target'] == 1) & (news['predictions'] == 0)]\n",
    "fakewrong = news.loc[(news['target'] == 0) & (news['predictions'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "realcorrect.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3b183",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakecorrect.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6eb316",
   "metadata": {},
   "outputs": [],
   "source": [
    "realwrong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586215da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakewrong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea6bdf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "realcorrect['top_word'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90936a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakecorrect['top_word'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "realwrong['top_word'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66468db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakewrong['top_word'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac637bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud1 = WordCloud(width=400, height=400, background_color='black',\n",
    "                      stopwords=[\"say\"], max_words=35)\n",
    "\n",
    "wordcloud1.generate(str(realcorrect['lemmatized']))\n",
    "\n",
    "wordcloud1.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud2 = WordCloud(width=400, height=400, background_color='white',\n",
    "                      stopwords=[\"said\"], max_words=35)\n",
    "\n",
    "wordcloud2.generate(str(fakecorrect['lemmatized']))\n",
    "\n",
    "wordcloud2.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa5d0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud3 = WordCloud(width=400, height=400, background_color='orange',\n",
    "                      stopwords=[\"said\"], max_words=35)\n",
    "\n",
    "wordcloud3.generate(str(realwrong['lemmatized']))\n",
    "\n",
    "wordcloud3.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d59810",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud4 = WordCloud(width=400, height=400, background_color='skyblue',\n",
    "                      stopwords=[\"said\"], max_words=35)\n",
    "wordcloud4.generate(str(fakewrong['lemmatized']))\n",
    "\n",
    "wordcloud4.to_image()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
